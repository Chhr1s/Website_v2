---
title: "Growth & Parallel Process Models in Lavaan"
author: "Christopher Loan"
date: "8/13/2020"
output: html_document
---

```{r setup, include=FALSE}
library(lavaan)
```
## During my MS, I worked on a few manuscripts which used growth modeling. These topics are well supported, but sometimes the support is hard to find if you don't know where to look. After subscribing to the Lavaan support Google Group, I saw many people requesting growth model and/or parallel process model support. I responded back to people a few times, but I figured it was just better to put a perminant link up showing the steps I took to fit, compare, and interpret growth models in Lavaan (one of my favorite statistical packages/software I've ever used!)

### Note: I will expect you understand lavaan somewhat, or at least can refer to Yve's (or others') work on syntax. I will give *brief* mentions to various syntax elements.

# Step 0: Simulating Data

## In order to complete this project, I simulated some data. For simplicity, I actually used lavaan which gives me 500 observations by default.

### It was easist to just make data that would fit a parallel process model, with that being our end goal anyways. Specifically, I made two variables which were "collected" across 4 waves (assumed to be equal in distance; x1 = x @ time 1, x2 = x @ time 2, etc.).  Here's the code I used to specify the data:

```{r}

growth_mod <- 
  '
## intercept & slope growth terms for X
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

## intercept, slope, & quadratic terms for Y
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4

## (ignore this for now, 
## we will come back to it during interpretation)

sX ~ 0.45*iX
sY ~ -0.2*iY

sY ~~ 1*sY
y4 ~~ 1.2*y4

outcome ~~ 3*outcome

iX + iY ~ 2*1
sX ~ 1*1
sY ~ -1*1
qY ~ -2*1

outcome ~ 0*iX + 0*iY + 2*sX + 3*sY + 1*qY

  '
```
# FIX THIS

## Variable `{x}` is changing linearly over time and variable `{y}` is changing quadratically. Quadratic terms represent the rate of change of the slope, also known as the acceleration. Before I go on, I wanted to help frame what growth models are telling you with an easy example: driving a car. 

# Growth Modeling in a (teeny, tiny) nutshell

## Let's pretend we took measures of peer cigarette use among adolescents `{Cig}` at 4 time points, each spaced 1 year apart.
   *   `{iCig}` represents baseline cigarette use,
   *   `{sCig}` represents rate of linear increase in cigarette use,
   *   `{qCig}` represents rate of quadratic increase in cigarette use (i.e., acceleration & decceleration). 
   
## `{lavaan}` essentially parses this distance `{D}` apart for each person into latent intercept, slope, and quadratic (or more if you ask it to) terms, which can be graphed as equations for each person. We also gain statistical tests of if these latent terms are related. i.e., did those who had a higher starting distance (i.e., a high predicted latent `{iD}`) have a lower speed than those who had lower predicted starting distances?



```{r}
sim_growth_dat <- simulateData(model = growth_mod, 
                               model.type = "growth", 
                               seed = 82020, 
                               orthogonal = F,
                               auto.cov.y = T, 
                               auto.var = T
                               )#,
                               # skewness = 0, #Default
                               # kurtosis = 0, #Default
                               # return.fit	= T,
                               # meanstructure = T,
                               # orthogonal = F, 
                               # standardized = T)

```
# Fit statistics

## It's good practice to have a set of fit statistcs specified *a priori* so you aren't cherry picking good and bad results. Here's a spread of them I've selected:
```{r}
selected_fit_stats <-   c("chisq.scaled",
                         "pvalue.scaled",
                         "df.scaled",
                         "cfi.scaled",
                         "rmsea.scaled",
                         "rmsea.pvalue.scaled",
                         "srmr"
                         )
```
# Assessing Functional Form 

## time to first fit the model to an intercept-only model (i.e., no growth observed), then look at linear growth (linear + intercept), then quadratic growth (quadratic + linear + intercept).

```{r}
int_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4

  '
int_x_fit <- growth(model = int_x_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

int_x_fit_stats<-fitmeasures(int_x_fit, selected_fit_stats) %>% data.frame()
int_x_fit_stats
```

```{r}
int_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  '
int_y_fit <- growth(model = int_y_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

int_y_fit_stats<-fitmeasures(int_y_fit, selected_fit_stats) %>% data.frame()
int_y_fit_stats
```

## Neither of those fit well, so we can add a higher-order fit term. Specifically, we will check how well a linear-only growth model fits for the `{x}` & `{y}` variables

```{r}
linear_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

  '
linear_x_fit <- growth(model = linear_x_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

linear_x_fit_stats<-fitmeasures(linear_x_fit, selected_fit_stats) %>% data.frame()
linear_x_fit_stats
```

```{r}
linear_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
  '
linear_y_fit <- growth(model = linear_y_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

linear_y_fit_stats<-fitmeasures(linear_y_fit, selected_fit_stats) %>% data.frame()
linear_y_fit_stats
```
## Looks like the linear `{x}` fits very well, but not `{y}`. We should check if the quadratic growth models fit well. We'll do both variables, in case a quadratic model fits the `{x}` better than the linear model for `{x}`.

```{r}
quad_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
qX =~ 0*x1 + 1*x2 + 4*x3 + 9*x4


  '
quad_x_fit <- growth(model = quad_x_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

quad_x_fit_stats <- fitmeasures(quad_x_fit, selected_fit_stats)
quad_x_fit_stats
```

```{r}
quad_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4


  '
quad_y_fit <- growth(model = quad_y_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

quad_y_fit_stats <- fitmeasures(quad_y_fit, selected_fit_stats)
quad_y_fit_stats
```

## It's clear that only the `{x}` linear model was a good one by all fit statistics. When using real—world data—rather than data simulated to be specific functional forms—this may or may not be the case. If that's the case, there are resources about comparing models. One popular way is through likelihood-ratio tests or chi-square difference tests. 

# Parallel process model 

## Now that we are fairly confident in functional forms, let's model them together
```{r}
full_model <- 
  '
## intercept & slope growth terms for X
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

## intercept, slope, & quadratic terms for Y
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4

sX ~ a1*iX + iY
sY ~ iX + a2*iY
qY ~ a3*sY + sX + iX + iY 

sY ~~ sX
iY ~~ iX

outcome ~ iX + b1*sX + iY + b2*sY + b3*qY

iX_sX_outcome := a1*b1
iY_sY_outcome := a2*b2
sY_qY_outcome := a3*b3
  '

full_fit <- growth(model = full_model, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)
full_fit_stats <- fitmeasures(full_fit, selected_fit_stats)
full_fit_stats
```

# Full Model Fit

## We can see that this model fits well across all fit statistics chosen. This model is deemed acceptable and we can now interpret the model results
```{r}
summary(full_fit,
        stand = T,
        rsq = T)
```
# Diagram
```{r eval=FALSE, include=FALSE}
semPlot::semPaths(full_fit, edge.label.cex = .6, what = "std", whatLabels = "std",
         label.prop=0.9, edge.label.color = "black", rotation = 1, #layout = "circle",
         equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5, 
         edge.width = 0.5, shapeMan = "rectangle", shapeLat = "ellipse", intercepts = F, #structural = T,
         shapeInt = "triangle", sizeMan = 4, sizeInt = 2, sizeLat = 4, 
         curve=2, unCol = "#070b8c", title = T)
title("Full Model")
```
## Direct Paths / Regressions
### The model shows that the variable `{outcome}` is predicted significantly by linear slopes of both `{x}` and `{y}` (i.e., `{sX}` & `{sY}`), as well as the `{qY}`. Baseline levels of both variables (`{iX}` & `{iY}`) did not significantly predict the outcome. 

###  We observed a significant indirect effect from baseline `{iX}` through `{sX}` to the `{outcome}`, which is likely why we do not see a direct effect of `{iX}` on the `{outcome}`. The same process is observed in `{Y}`. There was no indirect effect from `{sY}` to `{qY}` on `{outcome}`

## Covariances 
### 

## Means / Intercepts

# Bootstrapping

## Bootstrapping is a resampling method (with replacement) that is appreciated for building confidence intervals around parameter estimates of indirect effects, at least in part, because they do not make distributional assumptions about the indirect effect. This creates a more reliable test than standard significance tests. A quick Google Scholar search can inform you on this topic better than I can, but I used this to test the indirect effects using 5000 simulated data sets.

```{r}
final_fit_boot<- growth(full_fit, data = sim_growth_dat, estimator = "ML",
                        meanstructure = T,
                        se = "bootstrap", bootstrap = 5000,
                        parallel = "multicore")

parameterEstimates(final_fit_boot, level = .95, boot.ci.type = "bca.simple", output = "pretty", stand = T)
```

# Plotting

```{r}

```
## Factor Scores

```{r}
plot_dat <- lavPredict(full_fit) %>% data.frame()
plot_dat
```