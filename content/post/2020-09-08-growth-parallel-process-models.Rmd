---
title: Growth & Parallel Process Models
author: Christopher Loan
date: '2020-09-08'
slug: growth-parallel-process-models
categories:
  - Structural Equation Modeling
tags:
  - SEM
  - Structural Equation Modeling
description: ''
topics: []
---

```{r setup, include=FALSE}
library(lavaan)
library(semPlot)
library(ggplot2)
library(tidyverse)
library(lavaanPlot)
```
# Introduction: 

The first project I worked on in graduate school was doing secondary data analysis on a six-wave longitudinal study. I developed some research questions and dove in, looking at changes from one wave to the next. Quickly, though, I began expressing questions about the growth and change of dynamic factors that I couldn't answer with the methods I knew. \n
My advisor directed me towards structural equation modeling—as this was her background—with focus on growth models. At first I felt unable to pursue these methods with the price point of the software that she used for SEM, untitl I found `{lavaan}`. \n
I had to do a *lot* of reading to figure out how to get versed in both SEM and the package, so I wanted to share a lavaan-specific tutorial in such methods here.\n
This project will look at growth of two factors over time, as most of the tutorials I have found explain only one factor (e.g., [the growth curve page on the `lavaan` website](https://lavaan.ugent.be/tutorial/growth.html)) and putting two of these together can be daunting to a new `R` or `lavaan` user.\n
Think of this more as a tutorial of the steps needed to take when fitting growth models in `lavaan`, rather than a tutorial solely on either the method or the software.\n
As a quick terminology aside: I refer to growth models where 2+ factors are modeled "in parallel over time" as parallel process models (PPMs), but they are sometimes referred to differently. \n

#### Notes: 

1. Being a SEM method, this assumes some familiarity with SEM, though I hope to present this in a way that is clear to as many people as possible.

2. I expect you to understand or to be able to refer to [`lavaan` syntax](https://lavaan.ugent.be/tutorial/index.html) for basic syntax/interpretation.

3. Consider what I give you the nuts and bolts in how to make this work, and you should refer to other resources for more in depth interpretations. 

# Simulating Data for Demonstration

In order to complete this project, I simulated some data (n = 500). For simplicity, I actually used `lavaan` to make my data, but other software can do this too. \n
I am reserving the code for this until the end, in case you would like to know how I did that, or if you're curious to see how closely our PPM approximates the structure of the data we made. \n
Specifically, I made two variables across 4 waves (growth modeling assumes time to be equally spaced). \n
I named the variables x1 = x at time 1, x2 = x at time 2, etc. & y1 = y at time 1, etc.

```{r include=FALSE}

growth_mod <- 
  '
## intercept & slope growth terms for X
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

## intercept, slope, & quadratic terms for Y
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4

## (ignore this for now, we will come back to
## it during interpretation)

sX ~ 0.45*iX
sY ~ -0.2*iY

sY ~~ 1*sY
y4 ~~ 1.2*y4

outcome ~~ 3*outcome

iX + iY ~ 2*1
sX ~ 1*1
sY ~ -1*1
qY ~ -2*1

outcome ~ 0*iX + 0*iY + 2*sX + 3*sY + 1*qY

  '
```

```{r include=FALSE}
sim_growth_dat <- simulateData(model = growth_mod, 
                               model.type = "growth", 
                               seed = 82020, 
                               orthogonal = F,
                               auto.cov.y = T, 
                               auto.var = T
                               )#,
                               # skewness = 0, #Default
                               # kurtosis = 0, #Default
                               # return.fit	= T,
                               # meanstructure = T,
                               # orthogonal = F, 
                               # standardized = T)

```
# Fit statistics

It's good practice to have a set of fit statistcs specified *a priori* so you aren't cherry picking good and bad results. Here's a spread of them I've selected:
```{r}
selected_fit_stats <-   c("chisq.scaled",
                         "df.scaled",
                         "pvalue.scaled", ## ideally n.s.
                         "cfi.scaled", ## ideally ≥ 0.95
                         "rmsea.scaled", ## ideally ≤ 0.05
                         "rmsea.pvalue.scaled", ## ideally n.s.
                         "srmr" ## ideally < 0.08
                         )
```
# Steps for a PPM:

1. Assess fit of various models with different functional forms (i.e., no growth, linear growth, quadratic growth, etc.). 
2. Compare the fit of nested models.
3. Repeat

Youa can calculate fit on models where the highest term is x^(n-2)

Time to first fit the model to an intercept-only model (i.e., no growth observed), then look at linear growth (linear + intercept), then quadratic growth (quadratic + linear + intercept).

```{r}
int_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4

  '
int_x_fit <- growth(model = int_x_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

int_x_fit_stats<-fitmeasures(int_x_fit, selected_fit_stats) %>% data.frame()
int_x_fit_stats
```

```{r}
int_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  '
int_y_fit <- growth(model = int_y_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

int_y_fit_stats<-fitmeasures(int_y_fit, selected_fit_stats) %>% data.frame()
int_y_fit_stats
```

Neither of those fit well, so we can add a higher-order fit term. Specifically, we will check how well a linear-only growth model fits for the `{x}` & `{y}` variables

```{r}
linear_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

  '
linear_x_fit <- growth(model = linear_x_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

linear_x_fit_stats<-fitmeasures(linear_x_fit, selected_fit_stats) %>% data.frame()
linear_x_fit_stats
```

```{r}
linear_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
  '
linear_y_fit <- growth(model = linear_y_mod,
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

linear_y_fit_stats<-fitmeasures(linear_y_fit, selected_fit_stats) %>% data.frame()
linear_y_fit_stats
```

Looks like the linear `{x}` fits very well, but not `{y}`. We should check if the quadratic growth models fit well. We'll do both variables, in case a quadratic model fits the `{x}` better than the linear model for `{x}`.\n

As a note: Quadratic terms represent the rate of change of the slope on average across the waves. This is synonymous with acceleration, though this terminology is not often used; 'quadratic growth' is typically preferred 

```{r}
quad_x_mod <- 
  '
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
qX =~ 0*x1 + 1*x2 + 4*x3 + 9*x4


  '
quad_x_fit <- growth(model = quad_x_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

quad_x_fit_stats <- fitmeasures(quad_x_fit, selected_fit_stats)
quad_x_fit_stats
```

```{r}
quad_y_mod <- 
  '
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4


  '
quad_y_fit <- growth(model = quad_y_mod, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)

quad_y_fit_stats <- fitmeasures(quad_y_fit, selected_fit_stats)
quad_y_fit_stats
```

It's clear that only the `{x}` linear model was a good one by all fit statistics. When using real—world data—rather than data simulated to be specific functional forms—this may or may not be the case. If that's the case, there are resources about comparing models. One popular way is through likelihood-ratio tests or chi-square difference tests. 

# Parallel process model 

Now that we are fairly confident in functional forms, let's model them together
```{r}
full_model <- 
  '
## intercept & slope growth terms for X
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

## intercept, slope, & quadratic terms for Y
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4

sX ~ a1*iX + iY
sY ~ iX + a2*iY
qY ~ a3*sY + sX + iX + iY 

sY ~~ sX
iY ~~ iX

outcome ~ iX + b1*sX + iY + b2*sY + b3*qY

iX_sX_outcome := a1*b1
iY_sY_outcome := a2*b2
sY_qY_outcome := a3*b3
  '

full_fit <- growth(model = full_model, 
                     estimator = 'MLR',
                     data = sim_growth_dat,
                     meanstructure = T)
full_fit_stats <- fitmeasures(full_fit, selected_fit_stats)
```

# Full Model Fit

We can see that this model fits well across all fit statistics chosen. This model is deemed acceptable and we can now interpret the model results
```{r}
summary(full_fit,
        stand = T,
        rsq = T)
```

```{r eval=T, include=T}
# lavaanPlot(model = full_fit, coefs = T, stars = T,
#            covs = T)
# edge.label.cex = .6, what = "std", whatLabels = "std",
#          label.prop=0.9, edge.label.color = "black", rotation = 1, #layout = "circle",
#          equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5, 
#          edge.width = 0.5, shapeMan = "rectangle", shapeLat = "ellipse", intercepts = F, #structural = T,
#          shapeInt = "triangle", sizeMan = 4, sizeInt = 2, sizeLat = 4, 
#          curve=2, unCol = "#070b8c", title = T)
# title("Full Model")
```
# Direct Paths / Regressions
The model shows that the variable `{outcome}` is predicted significantly by linear slopes of both `{x}` and `{y}` (i.e., `{sX}` & `{sY}`), as well as the `{qY}`. Baseline levels of both variables (`{iX}` & `{iY}`) did not significantly predict the outcome. \n

We observed a significant indirect effect from baseline `{iX}` through `{sX}` to the `{outcome}`, which is likely why we do not see a direct effect of `{iX}` on the `{outcome}`. The same process is observed in `{Y}`. There was no indirect effect from `{sY}` to `{qY}` on `{outcome}`

### Covariances 
[ADD MATERIAL]

### Means / Intercepts

# Bootstrapping

Bootstrapping is a resampling method (with replacement) that is appreciated for building confidence intervals around parameter estimates of indirect effects, at least in part, because they do not make distributional assumptions about the indirect effect. This creates a more reliable test than standard significance tests. A quick Google Scholar search can inform you on this topic better than I can, but I used this to test the indirect effects using 5000 simulated data sets.

```{r warning=FALSE, cache = T}
final_fit_boot<- growth(full_fit, data = sim_growth_dat, estimator = "ML",
                        meanstructure = T,
                        se = "bootstrap",
                        bootstrap = 5000,
                        parallel = "multicore")

parameterEstimates(final_fit_boot,
                   level = .95,
                   boot.ci.type = "bca.simple",
                   stand = T)[64:66,c(4,5,9,10)]
```

The model converged nearly every time without issue. Of our 5000 bootstrap draws, only 7 were not successful; this is not shown here, but is if you call `{summary(full_fit, fit.measures = T)}`, which I didn't do for brevity. \n

Furthermore, we find the same results as we did regarding the indirect effect as without bootstrapping. These are the results I trust more, especially since we used the BCa simple variant used in `{lavaan}`. As a note, you should *not* trust the p-values from this parameterEstimates() if you go with anything but the default for `boot.ci.type =` if you use any `boot.ci.type` other than `norm`, as that is how the p-value is calculated. You can see a discussion between me & the lavaan Google Group about this if you do some digging. It may be changed in the future, but is not of yet (to my knowledge). Simply report Confidence Intervals (95% in my case).



# Plotting

We can plot the growth of these traits, as well with `{ggplot2}` \n
First we extract factor scores & save this as a data frame, then we can plot.\n
For a linear model, this is pretty simple:

```{r}
plot_dat <- lavPredict(full_fit) %>% data.frame()

plot_X <- ggplot(data=plot_dat) +
scale_x_continuous(name = "Timepoint",
                 limits=c(0,3),
                 breaks = c(0, 1, 2, 3),
                 labels = c('1', '2', '3', '4')
                 ) +
scale_y_continuous(name = "X",
                 limits=c(0,30)
                 )+
  geom_abline(data=plot_dat, 
              mapping=aes(slope=sX, 
                          intercept=iX), 
              color = "lightblue") + 
  theme_minimal()
plot_X
```

For a quadratic model, this is more challenging, but here's some code to do it:

```{r}
plot_dat <- lavPredict(full_fit) %>% data.frame()

qY <- plot_dat$qY
sY <- plot_dat$sY
iY <- plot_dat$iY

test <- function(y) {qY*y ^ 2 + sY*y + iY}



nrow(plot_dat)
plot_dat_2<-data.frame(`T1` = test(0), 
                       `T2` = test(1), 
                       `T3` = test(2), 
                       `T4` = test(3), 
                       row.names = 1:nrow(plot_dat))

names(plot_dat) <- c('1', '2', '3', '4') 

test0 <-test(0)
test1<-test(1)
test2<-test(2)
test3<-test(3)

Y_df <- matrix(c(test0, 
                 test1, 
                 test2, 
                 test3), 
               ncol = 1)

df_long_wave <- matrix(c(rep(1,500), 
                        rep(2, 500),
                        rep(3, 500), 
                        rep(4, 500)), 
                      ncol = 1)


plot_dat_2 <- data.frame(id = matrix(
  rep(1:500,4), ncol = 1),
  Timepoint = df_long_wave,
  Y = Y_df)


plot_Y <- plot_dat_2 %>% ggplot(aes(x = Timepoint, 
                          y = Y, 
                          group = id)) +
 geom_smooth(formula = y ~ x + I(x^2), 
             method = "lm", se = F)+
  theme_minimal() 

plot_Y
```

# Looking at the simulation call

```{r eval=FALSE}

growth_mod <- 
  '
## intercept & slope growth terms for X
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4

## intercept, slope, & quadratic terms for Y
iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4

## (ignore this for now, we will come back to
## it during interpretation)

sX ~ 0.45*iX
sY ~ -0.2*iY

sY ~~ 1*sY
y4 ~~ 1.2*y4

outcome ~~ 3*outcome

iX + iY ~ 2*1
sX ~ 1*1
sY ~ -1*1
qY ~ -2*1

outcome ~ 0*iX + 0*iY + 2*sX + 3*sY + 1*qY

  '
```


In the data I simulated, Variable `{x}` is changing linearly over time and variable `{y}` is changing quadratically. 


```{r eval=FALSE}
sim_growth_dat <- simulateData(model = growth_mod, 
                               model.type = "growth", 
                               seed = 82020, 
                               orthogonal = F,
                               auto.cov.y = T, 
                               auto.var = T
                               )

```